**海量小文件带来的问题**

**海量小文件带来的问题**

1、海量数据的性能问题。这是64K写的性能测试，一开始的1万到4亿个对象，这个时候会发现写入的性能逐步下降，从开始有15K的oes到下降到2.5，实际上下滑比例已经达到了80%多，海量小文件的冲击是非常大的。

究其原因，如果用的是Fd，底下的文件是要从Fd到dentry再到inode，superblock，1亿的文件，256B，它已经占了24G，小文件带来的是要直接操作磁盘，直接操作磁盘会导致性能下降，海量的小文件会破坏空间的连续性，会产生大量的随即读写。海量小文件会有大量源数据，性能还是不能得到流畅。

2、数据恢复效率问题， 数据在做恢复的过程中效率的问题，海量场景下恢复的速度，后者是前者的10倍，海量的小文件场景下，数据恢复的速度比较缓慢，而且效率很低，在此期间如果刚好不巧有业务请求过来，有可能会看到Slow Requses或是blocked，是存在风险的。

3、扩容、恢复以后集群的性能还会出现骤降的情况。如果做了扩容或是故障恢复，这个时候大量的小文件可能会被删除，可能会出现大量的碎片，这个碎片可能出现在磁盘上，如果不进行碎片的回收，里面系统的性能会出现骤降的情况，这是我们测的一组数据，前面很平稳，一旦扩容等恢复以后，这个比例会变大。

4、海量小文件的场景，数据的迁移效率比较低。可能想用数据冷热分层的方式把热数据移到冷的存储池，这个时候有大量的小文件在这个过程中会产生迁移，而且这个迁移的过程中前面也会有，迁移以后数据可能进行大量的删除操作。之前测试的4000万的小文件迁移消耗时间大于72个小时，算下来要两天多的时间。这种情况下迁移的效率太低了。



## 传统文件系统

大家知道传统的文件系统下，每个文件都要被创建对应的inode之类元数据，但是在海量文件场景下，传统FS已经无法承载如此多的元数据IO量以及如此庞大的元数据搜索计算量了，唯一的做法就是降低元数据量，那么势必就要降低文件实体的数量，所以这些文件系统无一例外的都是用了这样一种变通的方法，即在文件中再创建文件，比如一个64MB的大文件，比如其中可以包含16384个4KB的小文件，但是这个64MB的大文件只占用了1个inode，而如果存放4KB的文件的话，就需要16384个inode了。
作者：张冬链接：https://www.zhihu.com/question/26504749/answer/33012474来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。